{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1K3yrkuSM+UWyxNXE4YcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojabisht10/Deep-Learning/blob/main/UCS761_Lab6_Learning_to_Bend_a_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "3uhFhoJqh_sM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C8gqyveRhxsO"
      },
      "outputs": [],
      "source": [
        "# We only use numpy for numerical operations.\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input generation"
      ],
      "metadata": {
        "id": "pn5sB8siiHsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input data X\n",
        "# X represents a single feature whose value increases gradually\n",
        "\n",
        "X = np.linspace(0, 10, 50).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "NmDOsXukiC1u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation:\n",
        "# X represents an input variable that grows from 0 to 10.\n",
        "# This range is chosen so that we can see:\n",
        "# - fast growth in the beginning\n",
        "# - slower growth later\n",
        "# A straight line cannot fit both behaviors well."
      ],
      "metadata": {
        "id": "TUxWpD7ciJXf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target generation"
      ],
      "metadata": {
        "id": "FKuW9eFRiOuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create target y with a non-linear relationship\n",
        "# We add small noise so data looks realistic\n",
        "\n",
        "np.random.seed(0)\n",
        "noise = np.random.normal(0, 0.2, size=(50, 1))\n",
        "y = np.log(X + 1) + noise"
      ],
      "metadata": {
        "id": "sFuZb9tXiNC1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation:\n",
        "# The logarithmic function grows quickly at first and then slows down.\n",
        "# A linear model has only one slope, so it cannot fit this shape well."
      ],
      "metadata": {
        "id": "BjaOq8k5iQgs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decide the Model Shape"
      ],
      "metadata": {
        "id": "mg-SyYvViVFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We choose 3 hidden units.\n",
        "# Why more than 1?\n",
        "#   A single hidden unit cannot bend the curve enough.\n",
        "# Why not too many?\n",
        "#   Too many hidden units increase complexity and instability.\n",
        "# Three is a balanced choice for this simple dataset."
      ],
      "metadata": {
        "id": "GMqvuAjniVYK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Model Parameters"
      ],
      "metadata": {
        "id": "sxXmwWcSiZla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "\n",
        "# W1 maps input → hidden layer\n",
        "W1 = np.random.uniform(-1, 1, size=(1, 3))\n",
        "b1 = np.zeros((1, 3))\n",
        "\n",
        "# W2 maps hidden → output layer\n",
        "W2 = np.random.uniform(-1, 1, size=(3, 1))\n",
        "b2 = np.zeros((1, 1))"
      ],
      "metadata": {
        "id": "vwNYjK_MiSaE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation:\n",
        "# Weights control the shape and bending of the curve.\n",
        "# Biases shift the curve up or down.\n",
        "# Random initialization avoids symmetry."
      ],
      "metadata": {
        "id": "WT-4Q9O9icIt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Function"
      ],
      "metadata": {
        "id": "aPx-T4p5igNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(z):\n",
        "    # Keeps positive values and suppresses negative ones\n",
        "    return np.maximum(0, z)"
      ],
      "metadata": {
        "id": "w5NYshK0idst"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Slope"
      ],
      "metadata": {
        "id": "EHp2qZYqilR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_slope(z):\n",
        "    # Slope is 1 for positive values and 0 for negative values\n",
        "    return (z > 0).astype(float)"
      ],
      "metadata": {
        "id": "a8UZJOBUijps"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why slope matters:\n",
        "# Without slope, the model does not know how to change parameters.\n",
        "# Slopes allow error information to flow backward."
      ],
      "metadata": {
        "id": "vf_vBoDUinLu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass"
      ],
      "metadata": {
        "id": "3JWdDjLYir04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Linear transformation\n",
        "z1 = X @ W1 + b1\n",
        "\n",
        "# Step 2: Apply activation\n",
        "h = activation(z1)\n",
        "\n",
        "# Step 3: Output prediction\n",
        "y_hat = h @ W2 + b2"
      ],
      "metadata": {
        "id": "6HP6GKLEio7N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping to math:\n",
        "# z1 = XW1 + b1\n",
        "# h = activation(z1)\n",
        "# y_hat = hW2 + b2"
      ],
      "metadata": {
        "id": "5P8NmUzIi07a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error and loss"
      ],
      "metadata": {
        "id": "JN0CSU_2i48t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Error is the difference between prediction and actual value\n",
        "error = y_hat - y\n",
        "\n",
        "# Mean Squared Error loss\n",
        "loss = np.mean(error ** 2)\n",
        "\n",
        "print(\"Initial loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHpXs_Pei2H_",
        "outputId": "4832caf6-0f47-42eb-d803-bcde5e40f0bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 4.917843952937002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation:\n",
        "# Squaring makes large errors more expensive.\n",
        "# This forces the model to correct big mistakes strongly."
      ],
      "metadata": {
        "id": "eLuu7uIPi3z0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we need slopes?"
      ],
      "metadata": {
        "id": "1rMKgDNLi8i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To reduce loss, we must change parameters.\n",
        "# To know how to change them, we need slopes (gradients).\n",
        "# Gradients tell us whether a small change increases or decreases loss."
      ],
      "metadata": {
        "id": "bum7nxxyi7Jn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward flow"
      ],
      "metadata": {
        "id": "XCbfKG2Ri_7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient of loss w.r.t prediction\n",
        "dL_dy = 2 * error / len(X)\n",
        "\n",
        "# Gradients for output layer\n",
        "dL_dW2 = h.T @ dL_dy\n",
        "dL_db2 = np.sum(dL_dy, axis=0, keepdims=True)\n",
        "\n",
        "# Error flowing into hidden layer\n",
        "dL_dh = dL_dy @ W2.T\n",
        "\n",
        "# Apply activation slope\n",
        "dL_dz1 = dL_dh * activation_slope(z1)\n",
        "\n",
        "# Gradients for first layer\n",
        "dL_dW1 = X.T @ dL_dz1\n",
        "dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "XPyaMrKKjA6I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation:\n",
        "# Hidden values affect output, so gradients depend on them.\n",
        "# Activation controls which neurons pass error backward."
      ],
      "metadata": {
        "id": "UeTBVBxCi-en"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter update rule"
      ],
      "metadata": {
        "id": "xvSl3Yu8jFa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "# Update parameters by moving opposite to the gradient\n",
        "W1 -= learning_rate * dL_dW1\n",
        "b1 -= learning_rate * dL_db1\n",
        "W2 -= learning_rate * dL_dW2\n",
        "b2 -= learning_rate * dL_db2"
      ],
      "metadata": {
        "id": "-hejul5jjDxb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is similar to the perceptron update rule.\n",
        "# We move parameters in the opposite direction of error."
      ],
      "metadata": {
        "id": "ZbB1am6qjHVB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "srSZ3iYEjLXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Forward pass\n",
        "    z1 = X @ W1 + b1\n",
        "    h = activation(z1)\n",
        "    y_hat = h @ W2 + b2\n",
        "\n",
        "    # Loss\n",
        "    error = y_hat - y\n",
        "    loss = np.mean(error ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    dL_dy = 2 * error / len(X)\n",
        "    dL_dW2 = h.T @ dL_dy\n",
        "    dL_db2 = np.sum(dL_dy, axis=0, keepdims=True)\n",
        "    dL_dh = dL_dy @ W2.T\n",
        "    dL_dz1 = dL_dh * activation_slope(z1)\n",
        "    dL_dW1 = X.T @ dL_dz1\n",
        "    dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update\n",
        "    W1 -= learning_rate * dL_dW1\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    W2 -= learning_rate * dL_dW2\n",
        "    b2 -= learning_rate * dL_db2\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkdOt-8jjI3_",
        "outputId": "9858411f-af6c-4477-bb12-0bf66ac917b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.8784\n",
            "Epoch 100, Loss: 0.0736\n",
            "Epoch 200, Loss: 0.0696\n",
            "Epoch 300, Loss: 0.0683\n",
            "Epoch 400, Loss: 0.0679\n",
            "Epoch 500, Loss: 0.0677\n",
            "Epoch 600, Loss: 0.0677\n",
            "Epoch 700, Loss: 0.0677\n",
            "Epoch 800, Loss: 0.0676\n",
            "Epoch 900, Loss: 0.0676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choice justification:\n",
        "# Epochs = 1000 gives enough time for learning to stabilize.\n",
        "# Learning rate = 0.01 is small enough to avoid instability."
      ],
      "metadata": {
        "id": "7tthhUA1jKs5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This workshop shows:\n",
        "# - Why linear models fail on curved data\n",
        "# - How hidden layers allow bending\n",
        "# - Why activation functions are needed\n",
        "# - How error flows backward step by step\n",
        "# Backpropagation is not magic, just organized slopes."
      ],
      "metadata": {
        "id": "LgYV4P1-jP2E"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}